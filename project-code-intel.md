---
layout: page
title: Safe and Explainable AI for Code 
---

<div style="font-family: 'Alata';">
<small>
Aftab Hussain<sup>1</sup>, Md Rafiqul Islam Rabin<sup>1</sup>, Mohammad Amin
Alipour<sup>1</sup>, Vincent J. Hellendoorn<sup>2</sup>, Bowen Xu<sup>3</sup>,
Omprakash Gnawali<sup>1</sup>, Sen Lin<sup>1</sup>, Toufique Ahmed<sup>4</sup>,
Premkumar Devanbu<sup>4</sup>, Navid Ayoobi<sup>1</sup>, David Lo<sup>5</sup>,
Sahil Suneja<sup>6</sup> 
<br> <font color="gray">
University of Houston<sup>1</sup>,
Carnegie Mellon University<sup>2</sup>, North Carolina State
University<sup>3</sup>, University of California, Davis<sup>4</sup>, Singapore
Management University<sup>5</sup>, IBM Research<sup>6</sup>
<br> Supported by <a href = "https://www.sri.com/">SRI International</a>, <a
href = "https://www.iarpa.gov/">IARPA</a>
<br> 2021 to present</font> 
<br><b><a href="../Projects/index.html#code-intel-menu">Return to Projects</a></b>
<br>
</div>


● This extensive project delves into two aspects: (1) the behavior (Explainable
AI for Code) and (2) the security vulnerabilities (Safe AI for Code) of massive
deep neural models in the coding domain. 

● The models we investigate range in size from millions to billions of
parameters; they include transformer-based Large Language Models (LLMs) like
Microsoft's **CodeBERT**, Salesforce's **CodeT5**, Meta's **PLBART**, **Llama2**,
and **CodeLlama**, BigCode's **StarCoder**, against attacks on software
development tasks.

● Our works include model probing and black box techniques that involve
fine-tuning the models on noisy and poisoned code datasets derived from
benchmark sources like Microsoft's CodeXGLUE, utilizing NVIDIA A100 GPUs. 

<hr class="special">
<div class="row">
    <center>
        <b>
            <span style="color: #404040; font-family: 'Alata', sans-serif; font-weight: 300;">Contributions in Safe AI for Code</span>
        </b>
    </center>
</div>
<hr class="special">

*NOTE: A dedicated page for the Safe AI for Code segment may be found
[here](http://babylon.cs.uh.edu/web/).*  

## [Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code](../project-params-embeds/index.html) 
<small><font color="gray"> 
AIware'24: 1st ACM International Conference on AI-powered Software,
co-located with the ACM International Conference on the Foundations of Software
Engineering (FSE), 2024, Porto de Galinhas, Brazil
</font> 

## [On Trojan Signatures in Large Language Models of Code](../project-trojan-sig/index.html) 
<small><font color="gray"> 
International Conference on Learning Representations Workshop on Secure and
Trustworthy Large Language Models (SeT LLM at ICLR '24), 2024, Vienna,
Austria</font> 

<hr class="special">
<div class="row">
    <center>
        <b>
            <span style="color: #404040; font-family: 'Alata', sans-serif; font-weight: 300;">Contributions in Explainable AI for Code</span>
        </b>
    </center>
</div>
<hr class="special">

## [A Study of Variable-Role-based Feature Enrichment in Neural Models of Code](../project-roles/index.html) 
<small><font color="gray"> 
InteNSE'23: The 1st International Workshop on Interpretability and
Robustness in Neural Software Engineering, co-located with the 45th
International Conference on Software Engineering, ICSE 2023, Melbourne,
Australia</font> 

## [Memorization and Generalization in Neural Code Intelligence Models](../project-mem-gen/index.html) 
<small><font color="gray"> 
Journal of Information and Software Technology, 2023
</font> 
