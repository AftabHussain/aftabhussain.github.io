---
layout: page
title: Safe and Explainable AI for Code 
---

<div style="font-family: 'Alata'; font-size: small;">
<span>Aftab Hussain<sup>1</sup>, Md Rafiqul Islam Rabin<sup>1</sup>, Mohammad Amin
Alipour<sup>1</sup>, Vincent J. Hellendoorn<sup>2</sup>, Bowen Xu<sup>3</sup>,
Omprakash Gnawali<sup>1</sup>, Sen Lin<sup>1</sup>, Toufique Ahmed<sup>4</sup>,
Premkumar Devanbu<sup>4</sup>, Navid Ayoobi<sup>1</sup>, David Lo<sup>5</sup>,
Sahil Suneja<sup>6</sup> <br></span>
<span style="color: gray;">University of Houston<sup>1</sup>,
Carnegie Mellon University<sup>2</sup>, North Carolina State
University<sup>3</sup>, University of California, Davis<sup>4</sup>, Singapore
Management University<sup>5</sup>, IBM Research<sup>6</sup>
<br> 
<br> <span class="material-symbols-outlined" style="font-size: 13pt;">account_balance</span>Supported by <a href = "https://www.sri.com/">SRI International</a>, <a
href = "https://www.iarpa.gov/">IARPA</a>
<br> 2021 to present</span>
<br>
<span class="material-symbols-outlined" style="font-size: 13pt; color: #d6ac16;">construction</span>  
Skills used:<span style="color: gray; font-size: small;"> Python, Pytorch, SciPy, Matplotlib, NumPy, C, Java, SQL, model finetuning, freezed model finetuning, model parameter analysis, data extraction, data manipulation, machine learning, cybersecurity</span> 
<br>
<br>
<a href="../Projects/index.html#code-intel-menu"><span class="material-symbols-outlined" style="color: #1ba2d6;">arrow_back</span><b>Return to Projects</b></a>
<br>
<br>
</div>

<style>
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  max-width: 100%;
  height: auto;
}
</style>

<img src="../images/projects/code-intel/ai-background.jpg" alt="drawing"/>


This comprehensive project investigating massive deep neural models of code consists of two components, each encompassing multiple works. The **Explainable AI** component focuses on the behavior of these models, and the **Safe AI for Code** component focuses on their security. A dedicated page for the Safe AI for Code component can be found [here](http://babylon.cs.uh.edu/web/). The subject models range in size from millions to billions of
parameters (100m to 15b+) -- they include transformer-based Large Language Models (LLMs) like
Microsoft's [**CodeBERT**](https://github.com/microsoft/CodeBERT), Salesforce's [**CodeT5 and CodeT5+**](https://github.com/salesforce/CodeT5), Meta's
[**Llama2**](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [**CodeLlama**](https://huggingface.co/docs/transformers/en/model_doc/code_llama), BigCode's [**StarCoder**](https://github.com/bigcode-project/starcoder), against attacks on
software development tasks including **defect detection**, **clone detection**,
and **text-to-code generation**. The techniques we deploy include model probing and black box approaches that involve fine-tuning the models on noise-induced and poisoned code data derived from benchmark datasets like Microsoft's [**CodeXGLUE**](https://github.com/microsoft/CodeXGLUE), utilizing NVIDIA A100 GPUs. 

Here are the works in this project: 

## [Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code](../project-params-embeds/index.html) 
<div style="font-family: 'Alata';">
    <span style="color: gray; font-size: small;">
    AIware'24: 1st ACM
    International Conference on AI-powered Software, co-located with the ACM
    International Conference on the Foundations of Software Engineering (FSE),
    2024, Porto de Galinhas, Brazil 
    <br></span>
    <span class="material-symbols-outlined" style="color: #1ba2d6;">label</span>
    <span style="color: #1ba2d6; font-size: small;">Safe AI for Code</span>
</div>

_____________


## [On Trojan Signatures in Large Language Models of Code](../project-trojan-sig/index.html) 
<div style="font-family: 'Alata';">
    <span style="color: gray; font-size: small;"> International Conference on
Learning Representations Workshop on Secure and Trustworthy Large Language
Models (SeT LLM at ICLR '24), 2024, Vienna, Austria <br></span>
    <span class="material-symbols-outlined" style="color: #1ba2d6;">label</span>
    <span style="color: #1ba2d6; font-size: small;">Safe AI for Code</span>
</div>

_____________


## [A Study of Variable-Role-based Feature Enrichment in Neural Models of Code](../project-roles/index.html) 
<div style="font-family: 'Alata';">
    <span style="color: gray; font-size: small;">InteNSE'23: The 1st International Workshop on Interpretability and
Robustness in Neural Software Engineering, co-located with the 45th
International Conference on Software Engineering, ICSE 2023, Melbourne,
Australia  <br></span>
    <span class="material-symbols-outlined" style="color: #1ba2d6;">label</span>
    <span style="color: #1ba2d6; font-size: small;">Explainable AI for Code</span>
</div>

_____________


## [Memorization and Generalization in Neural Code Intelligence Models](../project-mem-gen/index.html) 
<div style="font-family: 'Alata';">
    <span style="color: gray; font-size: small;">Journal of Information and Software Technology, 2023
<br></span>
    <span class="material-symbols-outlined" style="color: #1ba2d6;">label</span>
    <span style="color: #1ba2d6; font-size: small;">Explainable AI for Code</span>
</div>

_____________

<p style="color:gray;font-size:8pt;"><small>Image:<a href="https://www.freepik.com/free-photo/virtual-projection-lights-forming-square-pattern-dark-background_13500430.htm#fromView=search&page=4&position=52&uuid=2464b102-c894-41db-ba6c-24ff2d6ce136" target="_blank">Freepik</a></small></p>
