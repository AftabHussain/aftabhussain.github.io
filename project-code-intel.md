---
layout: page
title: Safe and Explainable AI for Code 
---

<small>
Aftab Hussain<sup>1</sup>, Md Rafiqul Islam Rabin<sup>1</sup>, Mohammad Amin
Alipour<sup>1</sup>, Vincent J. Hellendoorn<sup>2</sup>, Bowen Xu<sup>3</sup>,
Omprakash Gnawali<sup>1</sup>, Sen Lin<sup>1</sup>, Toufique Ahmed<sup>4</sup>,
Premkumar Devanbu<sup>4</sup>, Navid Ayoobi<sup>1</sup>, David Lo<sup>5</sup>,
Sahil Suneja<sup>6</sup> <small>
<br> <font color="gray">
Software Engineering Research Group (University of Houston)<sup>1</sup>,
Carnegie Mellon University<sup>2</sup>, North Carolina State
University<sup>3</sup>, University of California, Davis<sup>4</sup>, Singapore
Management University<sup>5</sup>, IBM Research<sup>6</sup>
<br> Supported by <a href = "https://www.sri.com/">SRI International</a>, <a
href = "https://www.iarpa.gov/">Intelligence Advanced Research Project
Activity</a>
<br> 2021 to present</font> 
<br><b><a href="../Projects/index.html#code-intel-menu">Return to Projects</a></b>

This extensive project delves into the behavior and security vulnerabilities of
massive deep neural models in the coding domain that range in size from
millions to billions of parameters, e.g., transformer-based Large Language
Models (LLMs) like CodeBERT, PLBART, Llama, CodeLlama, StarCoder, against
attacks on software development tasks. Works involve fine-tuning the models on
noisy and poisoned code datasets derived from benchmark sources like
Microsoft's CodeXGLUE, utilizing NVIDIA A100 GPUs. The project comprises of the
following works:

## [Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code](../project-params-embeds/index.html) 
<small><font color="gray"> 
AIware'24: 1st ACM International Conference on AI-powered Software,
co-located with the ACM International Conference on the Foundations of Software
Engineering (FSE), Porto de Galinhas, Brazil
</font> 

## [On Trojan Signatures in Large Language Models of Code](../project-trojan-sig/index.html) 
<small><font color="gray"> 
International Conference on Learning Representations Workshop on Secure and
Trustworthy Large Language Models (SeT LLM at ICLR '24), 2024, Vienna,
Austria</font> 

## [A Study of Variable-Role-based Feature Enrichment in Neural Models of Code](../project-roles/index.html) 
<small><font color="gray"> 
InteNSE'23: The 1st International Workshop on Interpretability and
Robustness in Neural Software Engineering, co-located with the 45th
International Conference on Software Engineering, ICSE 2023, Melbourne,
Australia</font> 

____________________

<b>Download:
<br>[Technical Report](/documents/pubs/tech-report21-mem-gen-ci-models.pdf) 
<br><br>
<b>View:
<br>[Source code](https://github.com/AftabHussain/ICLR20-Great) 



_________________________

