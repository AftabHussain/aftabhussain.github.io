---
layout: page
title: Memorization and Generalization in Neural Code Intelligence Models 
---

Md Rafiqul Islam Rabin, Aftab Hussain, Vincent J. Hellendoorn, Mohammad Amin Alipour <small>
<br> <font color="gray">Software Engineering Research Group (University of
Houston), Carnegie Mellon University 
<br>Journal of Information and Software Technology, 2023
</font> 
<br><b><a href="../project-code-intel/index.html">Return to Safe and Explainable AI Projects</a></b>

Are deeplearning models memorizing your data instead of learning patterns?
Recent works suggest memorization risk is high with noisy training data. In
this exploration, we see the extent of memorization in neural code intelligence
models (models that can automatically do certain coding tasks), and provide
insights on how memorization may impact the learning behavior of such models.


- Evaluated the impact of adding training noise on the accuracy of Googleâ€™s GREAT family of code models for the variable-
misuse task. 

- Systematically induced input and output variable noise to the Py150 ETH VarMisuse dataset.

_________________________


<small>
<b>
Resources:
<br>
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0950584922001756">
<span class="material-symbols-outlined"> article </span>Paper - IST 2023 
<br>
<a href="https://arxiv.org/pdf/2106.08704">
<span class="material-symbols-outlined"> article </span>Paper - arXiv 2022 
<br>
<a href="https://github.com/AftabHussain/noise-gen_great-varmisuse">
<span class="material-symbols-outlined"> code_blocks </span>Source code - Github (Noise Inducer)
<br>
<a href="https://github.com/AftabHussain/save-trainstats_great-varmisuse">
<span class="material-symbols-outlined"> code_blocks </span>Source code - Github (Custom Finetuning Stats Generation)
</a>
</b>
</small>

